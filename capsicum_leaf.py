# -*- coding: utf-8 -*-
"""Capsicum_leaf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WqzrrWzPNJhbYc5vbtUlvabo7XaX2x9y
"""

from google.colab import drive
import os
import shutil
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
from tensorflow.keras.utils import load_img, img_to_array

# Mount Google Drive
from google.colab import drive  # Import the 'drive' function from 'google.colab'
drive.mount('/content/drive')

original_dataset_dir = '/content/drive/MyDrive/hackdata'
base_dir = '/content/pepper_split_dataset'
# Define train, validation, and test directories
train_dir = os.path.join(base_dir, 'train')
val_dir = os.path.join(base_dir, 'val')
test_dir = os.path.join(base_dir, 'test')


class_names = ['Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy']

# Create directories for train, validation, and test
os.makedirs(base_dir, exist_ok=True)
for folder in [train_dir, val_dir, test_dir]:
    os.makedirs(folder, exist_ok=True)
    for class_name in class_names:
        os.makedirs(os.path.join(folder, class_name), exist_ok=True)



# Split data
for class_name in class_names:
    class_path = os.path.join(original_dataset_dir, class_name)
    images = os.listdir(class_path)

    train, temp = train_test_split(images, test_size=0.3, random_state=42)
    val, test = train_test_split(temp, test_size=0.5, random_state=42)

    for img in train:
        shutil.copy(os.path.join(class_path, img), os.path.join(train_dir, class_name, img))
    for img in val:
        shutil.copy(os.path.join(class_path, img), os.path.join(val_dir, class_name, img))
    for img in test:
        shutil.copy(os.path.join(class_path, img), os.path.join(test_dir, class_name, img))

# Data generators
datagen = ImageDataGenerator(rescale=1./255)

train_generator = datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),
    batch_size=64,
    class_mode='categorical',
    shuffle=False
)

val_generator = datagen.flow_from_directory(
    val_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

test_generator = datagen.flow_from_directory(
    test_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

# Define a simple CNN model
cnn_model = models.Sequential([
    layers.Input(shape=(150, 150, 3)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
])

# Compile the model
cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Model summary
cnn_model.summary()

# Extract features from the CNN
train_features = cnn_model.predict(train_generator)
val_features = cnn_model.predict(val_generator)
test_features = cnn_model.predict(test_generator)

# Get corresponding labels
train_labels = train_generator.classes
val_labels = val_generator.classes
test_labels = test_generator.classes

# Train an SVM classifier
svm = SVC(kernel='linear', probability=True)
svm.fit(train_features, train_labels)

# Evaluate SVM
svm_predictions = svm.predict(test_features)
print("SVM Classification Report:")
print(classification_report(test_labels, svm_predictions))

# Train a Random Forest classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(train_features, train_labels)

# Evaluate Random Forest
rf_predictions = rf.predict(test_features)
print("Random Forest Classification Report:")
print(classification_report(test_labels, rf_predictions))

# Define the model architecture (Update the final Dense layer to have 2 output units for binary classification)
cnn_model = models.Sequential([
    layers.Input(shape=(150, 150, 3)),  # Input size (150x150 RGB image)
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(2, activation='softmax')  # Final layer with 2 output units (for two classes: healthy, bacterial spot)
])

# Compile the model
cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model if not already trained
cnn_model.fit(train_generator, epochs=10, validation_data=val_generator)

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report

# Step 1: Ensure test data is structured correctly
if hasattr(test_generator, 'classes'):
    print(f"âœ… Test generator contains {len(test_generator.classes)} samples.")

# Step 2: Generate CNN Predictions
y_pred_cnn_prob = cnn_model.predict(test_generator)  # Get probability scores
y_pred_cnn = np.argmax(y_pred_cnn_prob, axis=1)  # Convert softmax probabilities to class labels

# Step 3: Get True Labels
true_labels = test_generator.classes  # Actual class labels

# Step 4: Print Classification Report
print("\nðŸ“Š CNN Classification Report:")
print(classification_report(true_labels, y_pred_cnn))

# Step 1: Evaluate CNN on Test Data
cnn_loss, cnn_accuracy = cnn_model.evaluate(test_generator, verbose=1)
print(f"\nâœ… CNN Test Loss: {cnn_loss:.4f}")
print(f"âœ… CNN Test Accuracy: {cnn_accuracy:.4f}")

import matplotlib.pyplot as plt
from tensorflow.keras.utils import load_img, img_to_array

# Load and display the image
img_path = '/content/drive/MyDrive/hackdata/Pepper,_bell___Bacterial_spot/image (10).JPG'
img = load_img(img_path, target_size=(150, 150))

# Display the image
plt.imshow(img)
plt.axis('off')
plt.title("Input Image for Prediction")
plt.show()

# Preprocess the image for prediction
img_array = img_to_array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0)

# Extract features using CNN
single_feature = cnn_model.predict(img_array)

# Extract features using the layer before the final classification layer
feature_extractor = models.Model(inputs=cnn_model.inputs, outputs=cnn_model.layers[-2].output)  # Get the output of the second to last layer
single_feature = feature_extractor.predict(img_array)

# Predict using SVM
svm_prediction = svm.predict(single_feature)
svm_class = class_names[svm_prediction[0]]

# Predict using Random Forest
rf_prediction = rf.predict(single_feature)
rf_class = class_names[rf_prediction[0]]

# Display predictions
print(f"SVM Prediction: {svm_class}")
print(f"Random Forest Prediction: {rf_class}")

# Farmer recommendations
if svm_class == "Pepper,_bell___Bacterial_spot" or rf_class == "Pepper,_bell___Bacterial_spot":
    print("\nðŸŒ± **Farmer Recommendation**:")
    print("The bell pepper plant leaf is affected by bacterial spot.")
    print("- Remove infected leaves or plants to prevent spread.")
    print("- Avoid overhead watering; instead, water at the soil level.")
    print("- Use copper-based fungicides as per agricultural guidelines.")
    print("- Rotate crops to prevent recurring infections.")
else:
    print("\nðŸŒ± **Farmer Recommendation**:")
    print("The bell pepper plant leaf is healthy. Continue maintaining good agricultural practices.")
import matplotlib.pyplot as plt
from tensorflow.keras.utils import load_img, img_to_array

# Assuming cnn_model is the model you want to save
cnn_model.save('/content/capcicum_model.h5')

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix

# 1. Feature Importance Plot (Random Forest) - FIXED LABELS
def plot_feature_importance(model, feature_names, top_n=20):
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1][:top_n]  # Select top N important features

    plt.figure(figsize=(12, 6))
    plt.bar(range(top_n), importances[indices], align="center")
    plt.xticks(range(top_n), [feature_names[i] for i in indices], rotation=45, ha="right", fontsize=10)
    plt.xlabel("Feature Index")
    plt.ylabel("Importance")
    plt.title(f"Top {top_n} Feature Importance (RandomForestClassifier)")
    plt.show()

# Call this after training RandomForestClassifier
plot_feature_importance(rf, feature_names=[f"Feature {i}" for i in range(train_features.shape[1])], top_n=30)


# 2. ROC Curve & AUC (Fixing SVM issue)
def plot_roc_curve(y_true, y_pred_proba, model_name="Model", is_svm=False):
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba if is_svm else y_pred_proba[:, 1])
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(7, 5))
    plt.plot(fpr, tpr, color='blue', lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve for {model_name}')
    plt.legend()
    plt.show()

# Call this for SVM (pass `is_svm=True` to handle 1D array)
plot_roc_curve(test_labels, svm.decision_function(test_features), "SVM", is_svm=True)

# Call this for Random Forest
plot_roc_curve(test_labels, rf.predict_proba(test_features), "Random Forest")


# 3. Prediction Confidence Distribution
def plot_confidence_distribution(model_proba, model_name="Model"):
    confidences = np.max(model_proba, axis=1)

    plt.figure(figsize=(8, 5))
    sns.histplot(confidences, bins=20, kde=True, color="blue")
    plt.xlabel("Prediction Confidence")
    plt.ylabel("Frequency")
    plt.title(f"{model_name} Prediction Confidence Distribution")
    plt.show()

# Call this for SVM and RF
plot_confidence_distribution(svm.predict_proba(test_features), "SVM")
plot_confidence_distribution(rf.predict_proba(test_features), "Random Forest")


# 4. Feature Correlation Heatmap (Using Extracted Features)
def plot_feature_correlation(features):
    corr_matrix = np.corrcoef(features.T)

    plt.figure(figsize=(10, 8))
    sns.heatmap(corr_matrix, cmap="coolwarm", annot=False)
    plt.title("Feature Correlation Heatmap")
    plt.show()

# Call this using extracted features
plot_feature_correlation(train_features)

history = cnn_model.fit(train_generator, epochs=10, validation_data=val_generator)

"""# New section"""